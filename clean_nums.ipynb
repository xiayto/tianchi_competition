{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据进行合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n",
      "find_is_copy\n",
      "(8104368, 3)\n",
      "begin\n",
      "xxx\n",
      "finish\n",
      "(57298, 2795)\n",
      "totle time 136.9307188987732\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取数据\n",
    "part_1 = pd.read_csv('./input/meinian_round1_data_part1_20180408.txt',sep='$')\n",
    "part_2 = pd.read_csv('./input/meinian_round1_data_part2_20180408.txt',sep='$')\n",
    "part_1_2 = pd.concat([part_1,part_2])\n",
    "part_1_2 = pd.DataFrame(part_1_2).sort_values('vid').reset_index(drop=True)\n",
    "begin_time = time.time()\n",
    "print('begin')\n",
    "# 重复数据的拼接操作\n",
    "def merge_table(df):\n",
    "    df['field_results'] = df['field_results'].astype(str)\n",
    "    if df.shape[0] > 1:\n",
    "        merge_df = \" \".join(list(df['field_results']))\n",
    "    else:\n",
    "        merge_df = df['field_results'].values[0]\n",
    "    return merge_df\n",
    "# 数据简单处理\n",
    "print('find_is_copy')\n",
    "print(part_1_2.shape)\n",
    "is_happen = part_1_2.groupby(['vid','table_id']).size().reset_index()\n",
    "# 重塑index用来去重\n",
    "is_happen['new_index'] = is_happen['vid'] + '_' + is_happen['table_id']\n",
    "is_happen_new = is_happen[is_happen[0]>1]['new_index']\n",
    "\n",
    "part_1_2['new_index'] = part_1_2['vid'] + '_' + part_1_2['table_id']\n",
    "\n",
    "unique_part = part_1_2[part_1_2['new_index'].isin(list(is_happen_new))]\n",
    "unique_part = unique_part.sort_values(['vid','table_id'])\n",
    "no_unique_part = part_1_2[~part_1_2['new_index'].isin(list(is_happen_new))]\n",
    "print('begin')\n",
    "part_1_2_not_unique = unique_part.groupby(['vid','table_id']).apply(merge_table).reset_index()\n",
    "part_1_2_not_unique.rename(columns={0:'field_results'},inplace=True)\n",
    "print('xxx')\n",
    "tmp = pd.concat([part_1_2_not_unique,no_unique_part[['vid','table_id','field_results']]])\n",
    "# 行列转换\n",
    "print('finish')\n",
    "tmp = tmp.pivot(index='vid',values='field_results',columns='table_id')\n",
    "tmp.to_csv('./tmp.csv')\n",
    "print(tmp.shape)\n",
    "print('totle time',time.time() - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------移除数据缺失多的字段-----------\n",
      "移除之前总的字段数量 2795\n",
      "移除缺失数据的字段数量: 2415\n",
      "剩余的字段数量 380\n"
     ]
    }
   ],
   "source": [
    "# 清理缺失值大于阈值的函数\n",
    "def remain_feat(df,thresh=0.9):\n",
    "    exclude_feats = []\n",
    "    print('----------移除数据缺失多的字段-----------')\n",
    "    print('移除之前总的字段数量',len(df.columns))\n",
    "    num_rows = df.shape[0]\n",
    "    for c in df.columns:\n",
    "        num_missing = df[c].isnull().sum()\n",
    "        if num_missing == 0:\n",
    "            continue\n",
    "        missing_percent = num_missing / float(num_rows)\n",
    "        if missing_percent > thresh:\n",
    "            exclude_feats.append(c)\n",
    "    print(\"移除缺失数据的字段数量: %s\" % len(exclude_feats))\n",
    "    # 保留超过阈值的特征\n",
    "    feats = []\n",
    "    for c in df.columns:\n",
    "        if c not in exclude_feats:\n",
    "            feats.append(c)\n",
    "    print('剩余的字段数量',len(feats))\n",
    "\n",
    "    df = df[feats]\n",
    "    return df\n",
    "\n",
    "threshold = 0.95\n",
    "tmp = remain_feat(tmp, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 找出数字特征，进一步筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总特征数： 380\n",
      "数字特征数量： 219\n",
      "----------移除数据缺失多的字段-----------\n",
      "移除之前总的字段数量 219\n",
      "移除缺失数据的字段数量: 147\n",
      "剩余的字段数量 72\n"
     ]
    }
   ],
   "source": [
    "# 找出数字特征，30%以上是数字认为是数字\n",
    "def is_num(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        if(x == None):\n",
    "            x = 0\n",
    "        elif (x != x):\n",
    "            x = 0\n",
    "        else:\n",
    "            x = 1\n",
    "    except:\n",
    "        x = 0\n",
    "    return x\n",
    "\n",
    "def find_nums(df, threshold=0.3):\n",
    "    num_feature = []\n",
    "    print('总特征数：', df.shape[1])\n",
    "    for c in df.columns:\n",
    "        df_mask = df[c].apply(is_num)\n",
    "        if(df_mask.sum() / df[c].count() > threshold):\n",
    "            num_feature.append(c)\n",
    "    print('数字特征数量：', len(num_feature))\n",
    "    return num_feature\n",
    "\n",
    "def nums_df(df, remain_th, threshold=0.3):\n",
    "    num_feature = find_nums(df, threshold)\n",
    "    df_nums = remain_feat(df[num_feature], remain_th)\n",
    "    return df_nums\n",
    "\n",
    "num_remain_th = 0.8\n",
    "# 数值型特征缺失值少于大于0.8删除， 30%以上是数字认为是数字\n",
    "tmp_nums = nums_df(tmp, num_remain_th, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对数字型特征进行清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 对于有数字和其他值的，把第一个出现的数字切出来\n",
    "def search_num(x):\n",
    "    x_str = str(x)\n",
    "    pattern = re.compile(\"\\d+\\.?\\d*\")\n",
    "    match = re.search(pattern, x_str)\n",
    "    if(match != None):\n",
    "        return match.group()\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# 对于第0列，过缓小于50，过速度大于110\n",
    "def apply_0(x):\n",
    "    x_str = str(x)\n",
    "    if('过缓' in x_str):\n",
    "        x = 40\n",
    "    elif('过速' in x_str):\n",
    "        x = 110\n",
    "    return x\n",
    "\n",
    "# 切片找到数值\n",
    "tmp_nums = tmp_nums.applymap(search_num)\n",
    "# 对第0列进行特殊处理：\n",
    "tmp_nums.iloc[:,0] = tmp_nums.iloc[:,0].apply(apply_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  缺失值和离群点处理\n",
    "\n",
    "* 1、含表示正常的cell，填充均值\n",
    "* 2、大于3倍均值的离群点，可能是误输入，设为3倍均值\n",
    "* 3、对于缺失值置0，用nmf进行填充\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 判断是是否有数字，用于计算有多少个数字\n",
    "def has_value(x):\n",
    "    if x != x:\n",
    "        return 0\n",
    "    if x == None:\n",
    "        return 0\n",
    "    try:\n",
    "        float(x)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "# 有数字返回数字，用于求含nan的平均值\n",
    "def re_value(x):\n",
    "    if x != x:\n",
    "        return 0\n",
    "    if x == None:\n",
    "        return 0\n",
    "    try:\n",
    "        x = float(x)\n",
    "        return x\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# 用于计算方差\n",
    "def p_fang(c_mean):\n",
    "    def re_fang(x):\n",
    "        if x != x:\n",
    "            return 0\n",
    "        if x == None:\n",
    "            return 0\n",
    "        try:\n",
    "            float(x)\n",
    "            return (x-c_mean)*(x-c_mean)\n",
    "        except:\n",
    "            return 0\n",
    "    return re_fang\n",
    "\n",
    "# 计算均值\n",
    "def cal_mean(df, ci):\n",
    "    s = df.iloc[:,ci].apply(re_value).sum()\n",
    "    n = df.iloc[:,ci].apply(has_value).sum()\n",
    "    return s/n\n",
    "\n",
    "# 清除离群点\n",
    "def clean_out_dot(c_mean, n):\n",
    "    def out_dot_apply(x):\n",
    "        try:\n",
    "            x = float(x)\n",
    "            if(x > n*c_mean):\n",
    "                x = n*c_mean\n",
    "            return x\n",
    "        except:\n",
    "            return x\n",
    "    return out_dot_apply\n",
    "\n",
    "# 正常值赋均值\n",
    "def normal_apply(normal_list, c_mean):\n",
    "    def normal_app(x):\n",
    "        x = str(x)\n",
    "        for nor in normal_list:\n",
    "            if(nor in x):\n",
    "                x = c_mean\n",
    "                break\n",
    "        return x\n",
    "    return normal_app\n",
    "\n",
    "# 不能处理的点赋值nan\n",
    "def nan_apply(x):\n",
    "    try:\n",
    "        x = float(x)\n",
    "    except:\n",
    "        x = np.nan\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  清除离群点，正常值赋均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清除离群点前100007最大值 51.8\n",
      "清除离群点后100007最大值 2.01404259554\n"
     ]
    }
   ],
   "source": [
    "def out_dot(df, normal_list, n=3):\n",
    "    # 清除离群点：\n",
    "    print('清除离群点前100007最大值', df['100007'].apply(re_value).max())\n",
    "    for ci in range(df.shape[1]):\n",
    "        c_mean = cal_mean(df, ci)\n",
    "        df.iloc[:, ci] = df.iloc[:, ci].apply(clean_out_dot(c_mean, n))\n",
    "    print('清除离群点后100007最大值', df['100007'].apply(re_value).max())\n",
    "    \n",
    "    # 正常人填充平均值\n",
    "    for ci in range(df.shape[1]):\n",
    "        c_mean = cal_mean(df, ci)\n",
    "        df.iloc[:, ci] = df.iloc[:, ci].apply(normal_apply(normal_list, c_mean))\n",
    "        # 其他不能处理类型赋空\n",
    "        df.iloc[:, ci] = df.iloc[:, ci].apply(nan_apply)\n",
    "        \n",
    "    return df\n",
    "\n",
    "normal_list = ['阴性','正常','Normal','未见异常']\n",
    "# 大于3倍均值，令它为3倍均值\n",
    "tmp_nums = out_dot(tmp_nums, normal_list, 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 清除缺失特征太多的人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取train和test集\n",
    "train_df=pd.read_csv('./input/meinian_round1_train_20180408.csv',sep=',',encoding='gbk')\n",
    "test_df=pd.read_csv('./input/meinian_round1_test_a_20180409.csv',sep=',',encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清洗train集的label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_label(x):\n",
    "    x=str(x)\n",
    "    if '+' in x:#16.04++\n",
    "        i=x.index('+')\n",
    "        x=x[0:i]\n",
    "    if '>' in x:#> 11.00\n",
    "        i=x.index('>')\n",
    "        x=x[i+1:]\n",
    "    if len(x.split(sep='.'))>2:#2.2.8\n",
    "        i=x.rindex('.')\n",
    "        x=x[0:i]+x[i+1:]\n",
    "    if '未做' in x or '未查' in x or '弃查' in x or '不查' in x or '弃检' in x:\n",
    "        x=np.nan\n",
    "    if str(x).isdigit()==False and len(str(x))>4:\n",
    "        x=x[0:4]\n",
    "    return x\n",
    "\n",
    "def clean_label(x):\n",
    "    x=str(x)\n",
    "    if '+' in x:#16.04++\n",
    "        i=x.index('+')\n",
    "        x=x[0:i]\n",
    "    if '>' in x:#> 11.00\n",
    "        i=x.index('>')\n",
    "        x=x[i+1:]\n",
    "    if len(x.split(sep='.'))>2:#2.2.8\n",
    "        i=x.rindex('.')\n",
    "        x=x[0:i]+x[i+1:]\n",
    "    if '未做' in x or '未查' in x or '弃查' in x or '不查' in x or '弃检' in x:\n",
    "        x=np.nan\n",
    "    if str(x).isdigit()==False and len(str(x))>4:\n",
    "        x=x[0:4]\n",
    "    return x\n",
    "\n",
    "def data_clean(df):\n",
    "    label_list = ['收缩压','舒张压','血清甘油三酯','血清高密度脂蛋白','血清低密度脂蛋白']\n",
    "    for c in label_list:\n",
    "        df[c]=df[c].apply(clean_label)\n",
    "        df[c]=df[c].astype('float64')\n",
    "    df.dropna(axis=0, inplace=True, how='any')\n",
    "    return df\n",
    "\n",
    "train_df = data_clean(train_df)\n",
    "# 设置相同的index方便合并\n",
    "train_df.set_index('vid',inplace=True)\n",
    "test_df.set_index('vid', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp = pd.concat([train_df, tmp_nums], axis=1, join='inner').iloc[:,5:]\n",
    "test_tmp = pd.concat([test_df, tmp_nums], axis=1, join='inner').iloc[:,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清理之前的人数： 38192\n",
      "清理的人数是： 9429\n",
      "清理后的人数是： 28763\n"
     ]
    }
   ],
   "source": [
    "# 清洗缺失值太多的人：\n",
    "def clean_people(df, th):\n",
    "    print('清理之前的人数：', df.shape[0])\n",
    "    del_list = []\n",
    "    for r in range(df.shape[0]):\n",
    "        if(df.iloc[r,:].count() / df.shape[1] < th):\n",
    "            del_list.append(df.index[r])\n",
    "    print('清理的人数是：', len(del_list))\n",
    "    df = df.drop(del_list, axis=0)\n",
    "    print('清理后的人数是：', df.shape[0])\n",
    "    return df\n",
    "\n",
    "train_tmp = clean_people(train_tmp, 0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并train和test\n",
    "## 与主题向量结合之后，映射到0-100范围，做nmf的缺失值填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57298, 20)\n",
      "(38301, 92)\n"
     ]
    }
   ],
   "source": [
    "# 合并\n",
    "train_test_tmp = pd.concat([train_tmp, test_tmp], axis=0)\n",
    "\n",
    "# 读取主题向量\n",
    "topic_vec = pd.read_csv('./input/topic_vec20.csv')\n",
    "topic_vec.set_index('Unnamed: 0', inplace=True)\n",
    "print(topic_vec.shape)\n",
    "\n",
    "# 与train_test合并\n",
    "train_test_topic = pd.concat([train_test_tmp, topic_vec], axis=1, join='inner')\n",
    "print(train_test_topic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失值补0，其他值映射到0-100范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 找最小值\n",
    "def find_min(df, i):\n",
    "    min_data = 100000\n",
    "    for r in range(df.shape[0]):\n",
    "        if (df.iloc[r, i] < min_data) and (df.iloc[r, i] != 0):\n",
    "            min_data = df.iloc[r, i]\n",
    "    return min_data\n",
    "\n",
    "# 映射函数\n",
    "class nmf_apply(object):\n",
    "    def __init__(self, min_data, max_data, min_v, max_v):\n",
    "        self.min_data = min_data\n",
    "        self.max_data = max_data\n",
    "        self.min_v = min_v\n",
    "        self.max_v = max_v\n",
    "        \n",
    "    def set_min_max(self, min_data, max_data):\n",
    "        self.min_data = min_data\n",
    "        self.max_data = max_data\n",
    "    \n",
    "    def set_min_max_v(self, min_v, max_v):\n",
    "        self.min_v = min_v\n",
    "        self.max_v = max_v\n",
    "    \n",
    "    def nmf_app(self, x):\n",
    "        if(x == 0):\n",
    "            return x\n",
    "        else:\n",
    "            x = ((x - self.min_data) / (self.max_data - self.min_data)) * (self.max_v - self.min_v)\n",
    "        return x\n",
    "\n",
    "# 映射到 min_v 和 max_v 之间\n",
    "def nmf_fx(df, min_v, max_v):\n",
    "    nmf_i = nmf_apply(0, 0, min_v, max_v)\n",
    "    \n",
    "    for c in range(df.shape[1]):\n",
    "        max_data = df.iloc[:, c].max()\n",
    "        min_data = find_min(df, c)\n",
    "        nmf_i.set_min_max(min_data, max_data)\n",
    "        df.iloc[:, c] = df.iloc[:, c].apply(nmf_i.nmf_app)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_nmf = train_test_topic.fillna(0)\n",
    "tmp_nmf = nmf_fx(tmp_nmf, 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_nmf.to_csv('./input/tmp_nmf.txt', sep='\\t', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38301, 92)\n"
     ]
    }
   ],
   "source": [
    "print(tmp_nmf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nmf补全向量与原矩阵结合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38301, 92)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取nmf向量：\n",
    "nmf_train_after = pd.read_csv('./input/train_set_nmf_res.txt', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_nmf(x):\n",
    "    if x == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "mask = tmp_nmf.applymap(apply_nmf)\n",
    "temp = mask.values*nmf_train_after.values + tmp_nmf.values\n",
    "nmf_combine = pd.DataFrame(temp, index=tmp_nmf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 标准化处理\n",
    "\n",
    "def m_standard(df):\n",
    "    for c in df.columns:\n",
    "        c_std = df[c].std()\n",
    "        c_mean = df[c].mean()\n",
    "        df[c] = (df[c] - c_mean) / c_std\n",
    "    return df\n",
    "\n",
    "nmf_combine = m_standard(nmf_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 储存\n",
    "x_nmf = pd.concat([nmf_combine, train_df], axis=1, join='inner')\n",
    "predict_nmf = pd.concat([test_df, nmf_combine], axis=1, join='inner').iloc[:,5:]\n",
    "\n",
    "x_nmf.reset_index(inplace=True)\n",
    "predict_nmf.reset_index(inplace=True)\n",
    "\n",
    "x_nmf.to_csv('./input/new_nmf_train.csv', header=None, index=None)\n",
    "predict_nmf.to_csv('./input/new_nmf_test.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用均值填充 0\n",
    "\n",
    "# 填充0的apply函数\n",
    "def mean_apply(c_mean):\n",
    "    def m_apply(x):\n",
    "        if(x == 0):\n",
    "            x = c_mean\n",
    "        return x\n",
    "    return m_apply\n",
    "\n",
    "def count_not_zero(x):\n",
    "    if (x == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def f_mean(df):\n",
    "    for c in range(df.shape[1]):\n",
    "        c_mean = df.iloc[:,c].sum() / df.iloc[:,c].apply(count_not_zero).sum()\n",
    "        df.iloc[:,c] = df.iloc[:,c].apply(mean_apply(c_mean))\n",
    "    return df\n",
    "\n",
    "tmp_fill_mean = f_mean(tmp_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 做标准化处理：\n",
    "\n",
    "def m_standard(df):\n",
    "    for c in df.columns:\n",
    "        c_std = df[c].std()\n",
    "        c_mean = df[c].mean()\n",
    "        df[c] = (df[c] - c_mean) / c_std\n",
    "    return df\n",
    "\n",
    "tmp_fill_mean = m_standard(tmp_fill_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_mean = pd.concat([tmp_fill_mean, train_df], axis=1, join='inner')\n",
    "predict_mean = pd.concat([test_df, tmp_fill_mean], axis=1, join='inner').iloc[:,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean.reset_index(inplace=True)\n",
    "predict_mean.reset_index(inplace=True)\n",
    "\n",
    "x_mean.to_csv('./input/new_mean_train.csv', header=None, index=None)\n",
    "predict_mean.to_csv('./input/new_mean_test.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28763, 92)\n",
      "(28763,)\n"
     ]
    }
   ],
   "source": [
    "x_y_train = pd.concat([train_df, tmp_fill_mean], axis=1, join='inner')\n",
    "\n",
    "x_train = x_y_train.iloc[:,5:].values\n",
    "y_train = x_y_train.iloc[:,1].values\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "clf = XGBRegressor(\n",
    "        objective='reg:linear',\n",
    "        learning_rate=0.1,  # [默认是0.3]学习率类似，调小能减轻过拟合，经典值是0.01-0.2\n",
    "        gamma=0,  # 在节点分裂时，只有在分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。这个参数值越大，算法越保守。\n",
    "        subsample=0.8,  # 随机采样比例，0.5-1 小欠拟合，大过拟合\n",
    "        colsample_bytree=0.8,  # 训练每棵树时用来训练的特征的比例\n",
    "        reg_alpha=1,  # [默认是1] 权重的L1正则化项\n",
    "        reg_lambda=1,  # [默认是1] 权重的L2正则化项\n",
    "        max_depth=10,  # [默认是6] 树的最大深度，这个值也是用来避免过拟合的3-10\n",
    "        min_child_weight=1,  # [默认是1]决定最小叶子节点样本权重和。当它的值较大时，可以避免模型学习到局部的特殊样本。但如果这个值过高，会导致欠拟合。\n",
    "    )\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "xgb_params = clf.get_xgb_params()\n",
    "clf_xgb = xgb.train(xgb_params, dtrain, num_boost_round=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict = pd.concat([test_df, tmp_fill_mean], axis=1, join='inner').iloc[:, 5:].values\n",
    "x_predict = xgb.DMatrix(x_predict)\n",
    "res = clf_xgb.predict(x_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
